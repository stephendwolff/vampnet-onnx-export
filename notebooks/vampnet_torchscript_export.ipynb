{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VampNet TorchScript Export\n",
    "\n",
    "This notebook explores using TorchScript as an intermediate format for ONNX export.\n",
    "TorchScript can preserve more of the model's behavior and supports type annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Tuple, Optional, List\n",
    "import numpy as np\n",
    "import vampnet\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath('./')))\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Pretrained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load VampNet models\n",
    "interface = vampnet.interface.Interface(\n",
    "    codec_ckpt=\"../models/vampnet/codec.pth\",\n",
    "    coarse_ckpt=\"../models/vampnet/coarse.pth\",\n",
    ")\n",
    "\n",
    "coarse_model = interface.coarse\n",
    "print(f\"Coarse model type: {type(coarse_model)}\")\n",
    "print(f\"Has _orig_mod: {hasattr(coarse_model, '_orig_mod')}\")\n",
    "\n",
    "# Get the actual model\n",
    "if hasattr(coarse_model, '_orig_mod'):\n",
    "    actual_model = coarse_model._orig_mod\n",
    "else:\n",
    "    actual_model = coarse_model\n",
    "    \n",
    "print(f\"Actual model type: {type(actual_model)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding VampNet Model Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore model structure\n",
    "print(\"Model attributes:\")\n",
    "for attr in dir(actual_model):\n",
    "    if not attr.startswith('_') and not callable(getattr(actual_model, attr, None)):\n",
    "        print(f\"  {attr}: {getattr(actual_model, attr, 'N/A')}\")\n",
    "\n",
    "print(\"\\nModel methods:\")\n",
    "for attr in dir(actual_model):\n",
    "    if not attr.startswith('_') and callable(getattr(actual_model, attr, None)):\n",
    "        print(f\"  {attr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create TorchScript-Compatible Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VampNetTorchScriptWrapper(nn.Module):\n",
    "    \"\"\"\n",
    "    TorchScript-compatible wrapper with type annotations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vampnet_model):\n",
    "        super().__init__();\n",
    "        self.model = vampnet_model\n",
    "        self.n_codebooks: int = 4\n",
    "        self.vocab_size: int = 1024\n",
    "        self.mask_token: int = 1024\n",
    "        \n",
    "    def forward(self, codes: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass with generation.\n",
    "        \n",
    "        Args:\n",
    "            codes: Input codes [batch, n_codebooks, seq_len]\n",
    "            mask: Binary mask [batch, n_codebooks, seq_len]\n",
    "            \n",
    "        Returns:\n",
    "            Generated codes [batch, n_codebooks, seq_len]\n",
    "        \"\"\"\n",
    "        # Get logits from model\n",
    "        logits = self.model(codes)\n",
    "        \n",
    "        # Apply temperature (fixed at 1.0)\n",
    "        temperature: float = 1.0\n",
    "        logits = logits / temperature\n",
    "        \n",
    "        # Get predictions\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        # Apply mask\n",
    "        output = torch.where(mask.bool(), predictions, codes)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "# Create wrapper\n",
    "wrapper = VampNetTorchScriptWrapper(actual_model)\n",
    "wrapper.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test the Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test inputs\n",
    "test_codes = torch.randint(0, 1024, (1, 4, 100), device=device)\n",
    "test_mask = torch.randint(0, 2, (1, 4, 100), device=device)\n",
    "\n",
    "# Add some mask tokens\n",
    "test_codes[test_mask.bool()] = 1024\n",
    "\n",
    "print(f\"Test codes shape: {test_codes.shape}\")\n",
    "print(f\"Number of masked positions: {test_mask.sum().item()}\")\n",
    "\n",
    "# Test forward pass\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "        output = wrapper(test_codes, test_mask)\n",
    "    print(f\"✅ Forward pass successful! Output shape: {output.shape}\")\n",
    "    print(f\"Output differs at masked positions: {(output[test_mask.bool()] != test_codes[test_mask.bool()]).any().item()}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Forward pass failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Attempt TorchScript Tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to trace the model\n",
    "print(\"Attempting to trace the wrapper...\")\n",
    "\n",
    "try:\n",
    "    traced_model = torch.jit.trace(wrapper, (test_codes, test_mask))\n",
    "    print(\"✅ Tracing successful!\")\n",
    "    \n",
    "    # Test traced model\n",
    "    with torch.no_grad():\n",
    "        traced_output = traced_model(test_codes, test_mask)\n",
    "    \n",
    "    # Compare outputs\n",
    "    if torch.allclose(output, traced_output):\n",
    "        print(\"✅ Traced model produces identical output!\")\n",
    "    else:\n",
    "        max_diff = torch.max(torch.abs(output - traced_output)).item()\n",
    "        print(f\"⚠️ Traced model has differences. Max diff: {max_diff}\")\n",
    "        \n",
    "    # Save traced model\n",
    "    traced_model.save(\"traced_vampnet.pt\")\n",
    "    print(\"Saved traced model to traced_vampnet.pt\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Tracing failed: {e}\")\n",
    "    print(\"\\nThis is expected if the model has dynamic control flow.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Try TorchScript Scripting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try scripting instead\n",
    "print(\"Attempting to script the wrapper...\")\n",
    "\n",
    "try:\n",
    "    scripted_model = torch.jit.script(wrapper)\n",
    "    print(\"✅ Scripting successful!\")\n",
    "    \n",
    "    # Test scripted model\n",
    "    with torch.no_grad():\n",
    "        scripted_output = scripted_model(test_codes, test_mask)\n",
    "    \n",
    "    print(f\"Scripted output shape: {scripted_output.shape}\")\n",
    "    \n",
    "    # Save scripted model\n",
    "    scripted_model.save(\"scripted_vampnet.pt\")\n",
    "    print(\"Saved scripted model to scripted_vampnet.pt\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Scripting failed: {e}\")\n",
    "    print(\"\\nThe model likely contains operations not supported by TorchScript.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Simplified Approach: Extract Key Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplifiedVampNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified model that extracts key components from VampNet.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vampnet_model):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Try to extract key components\n",
    "        self.n_codebooks = 4\n",
    "        self.vocab_size = 1024\n",
    "        \n",
    "        # Extract embeddings if available\n",
    "        if hasattr(vampnet_model, 'embedding'):\n",
    "            self.embedding = vampnet_model.embedding\n",
    "            print(\"✅ Extracted embedding layer\")\n",
    "        else:\n",
    "            print(\"❌ No embedding layer found\")\n",
    "            \n",
    "        # Extract transformer if available\n",
    "        if hasattr(vampnet_model, 'transformer'):\n",
    "            self.transformer = vampnet_model.transformer\n",
    "            print(\"✅ Extracted transformer\")\n",
    "        elif hasattr(vampnet_model, 'net'):\n",
    "            self.transformer = vampnet_model.net\n",
    "            print(\"✅ Extracted net as transformer\")\n",
    "        else:\n",
    "            print(\"❌ No transformer found\")\n",
    "            \n",
    "        # Extract output projection if available\n",
    "        if hasattr(vampnet_model, 'classifier'):\n",
    "            self.classifier = vampnet_model.classifier\n",
    "            print(\"✅ Extracted classifier\")\n",
    "        else:\n",
    "            print(\"❌ No classifier found\")\n",
    "    \n",
    "    def forward(self, codes: torch.Tensor) -> torch.Tensor:\n",
    "        # This is a placeholder - would need actual implementation\n",
    "        return torch.randn(codes.shape[0], self.n_codebooks, codes.shape[2], self.vocab_size)\n",
    "\n",
    "\n",
    "# Try the simplified approach\n",
    "simplified = SimplifiedVampNet(actual_model)\n",
    "\n",
    "# List all modules in the actual model\n",
    "print(\"\\nActual model modules:\")\n",
    "for name, module in actual_model.named_modules():\n",
    "    if name:  # Skip the root module\n",
    "        print(f\"  {name}: {type(module).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Alternative: Export Individual Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of exporting the whole model, we can export individual operations\n",
    "# and reconstruct the model in ONNX\n",
    "\n",
    "class ComponentWrapper(nn.Module):\n",
    "    \"\"\"Wrapper for individual model components.\"\"\"\n",
    "    \n",
    "    def __init__(self, component):\n",
    "        super().__init__()\n",
    "        self.component = component\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.component(x)\n",
    "\n",
    "\n",
    "# Try to export the embedding layer\n",
    "if hasattr(actual_model, 'embedding'):\n",
    "    print(\"Attempting to export embedding layer...\")\n",
    "    embedding_wrapper = ComponentWrapper(actual_model.embedding)\n",
    "    \n",
    "    try:\n",
    "        # Create example input\n",
    "        example_tokens = torch.randint(0, 1024, (1, 4, 100))\n",
    "        \n",
    "        # Trace\n",
    "        traced_embedding = torch.jit.trace(embedding_wrapper, example_tokens)\n",
    "        \n",
    "        # Export to ONNX\n",
    "        torch.onnx.export(\n",
    "            traced_embedding,\n",
    "            example_tokens,\n",
    "            \"vampnet_embedding.onnx\",\n",
    "            input_names=['tokens'],\n",
    "            output_names=['embeddings'],\n",
    "            dynamic_axes={'tokens': {0: 'batch', 2: 'sequence'}},\n",
    "            opset_version=14\n",
    "        )\n",
    "        \n",
    "        print(\"✅ Successfully exported embedding layer!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to export embedding: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Custom TorchScript Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also define custom TorchScript operations\n",
    "@torch.jit.script\n",
    "def generate_tokens(logits: torch.Tensor, \n",
    "                   mask: torch.Tensor,\n",
    "                   codes: torch.Tensor,\n",
    "                   temperature: float = 1.0) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    TorchScript function for token generation.\n",
    "    \n",
    "    Args:\n",
    "        logits: Model output logits [batch, n_codebooks, seq_len, vocab_size]\n",
    "        mask: Binary mask [batch, n_codebooks, seq_len]\n",
    "        codes: Original codes [batch, n_codebooks, seq_len]\n",
    "        temperature: Temperature for scaling\n",
    "        \n",
    "    Returns:\n",
    "        Generated codes [batch, n_codebooks, seq_len]\n",
    "    \"\"\"\n",
    "    # Scale by temperature\n",
    "    scaled_logits = logits / temperature\n",
    "    \n",
    "    # Get predictions\n",
    "    predictions = torch.argmax(scaled_logits, dim=-1)\n",
    "    \n",
    "    # Apply mask\n",
    "    output = torch.where(mask.bool(), predictions, codes)\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "# Test the scripted function\n",
    "test_logits = torch.randn(1, 4, 100, 1024)\n",
    "result = generate_tokens(test_logits, test_mask, test_codes)\n",
    "print(f\"Scripted function output shape: {result.shape}\")\n",
    "\n",
    "# This function can be used as part of a larger TorchScript model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Hybrid Approach: TorchScript + ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridVampNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Hybrid model that uses TorchScript for complex operations\n",
    "    and can be exported to ONNX.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.n_codebooks = 4\n",
    "        self.vocab_size = 1024\n",
    "        self.d_model = 512\n",
    "        \n",
    "        # Use standard PyTorch layers that ONNX supports well\n",
    "        self.embedding = nn.Embedding(1025, self.d_model)  # +1 for mask token\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=self.d_model,\n",
    "                nhead=8,\n",
    "                dim_feedforward=2048,\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=6\n",
    "        )\n",
    "        self.output_proj = nn.Linear(self.d_model, self.vocab_size)\n",
    "        \n",
    "    def forward(self, codes: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size, n_codebooks, seq_len = codes.shape\n",
    "        \n",
    "        # Flatten codebooks\n",
    "        flat_codes = codes.view(batch_size, -1)\n",
    "        \n",
    "        # Embed\n",
    "        embeddings = self.embedding(flat_codes)\n",
    "        \n",
    "        # Transform\n",
    "        transformed = self.transformer(embeddings)\n",
    "        \n",
    "        # Project\n",
    "        logits = self.output_proj(transformed)\n",
    "        \n",
    "        # Reshape\n",
    "        logits = logits.view(batch_size, n_codebooks, seq_len, -1)\n",
    "        \n",
    "        # Use our scripted generation function\n",
    "        output = generate_tokens(logits, mask, codes, 1.0)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "# Create and test hybrid model\n",
    "hybrid = HybridVampNet()\n",
    "hybrid.eval()\n",
    "\n",
    "# Test\n",
    "with torch.no_grad():\n",
    "    hybrid_output = hybrid(test_codes, test_mask)\n",
    "print(f\"Hybrid model output shape: {hybrid_output.shape}\")\n",
    "\n",
    "# Try to export to ONNX\n",
    "try:\n",
    "    torch.onnx.export(\n",
    "        hybrid,\n",
    "        (test_codes, test_mask),\n",
    "        \"hybrid_vampnet.onnx\",\n",
    "        input_names=['codes', 'mask'],\n",
    "        output_names=['generated_codes'],\n",
    "        dynamic_axes={\n",
    "            'codes': {0: 'batch', 2: 'sequence'},\n",
    "            'mask': {0: 'batch', 2: 'sequence'},\n",
    "            'generated_codes': {0: 'batch', 2: 'sequence'}\n",
    "        },\n",
    "        opset_version=14\n",
    "    )\n",
    "    print(\"✅ Successfully exported hybrid model to ONNX!\")\n",
    "    \n",
    "    # Verify\n",
    "    onnx_model = onnx.load(\"hybrid_vampnet.onnx\")\n",
    "    onnx.checker.check_model(onnx_model)\n",
    "    print(\"✅ ONNX model verification passed!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to export hybrid model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "TorchScript can help with ONNX export in several ways:\n",
    "\n",
    "1. **Type Annotations**: Help TorchScript understand the model better\n",
    "2. **@torch.jit.script**: Can script individual functions for use in models\n",
    "3. **@torch.jit.trace**: Can trace models with fixed control flow\n",
    "4. **Hybrid Approach**: Combine TorchScript operations with ONNX-friendly layers\n",
    "\n",
    "However, the pretrained VampNet model is still too complex for direct export due to:\n",
    "- Custom layers and operations\n",
    "- Dynamic control flow\n",
    "- Complex attention mechanisms\n",
    "\n",
    "The best approaches are:\n",
    "1. Create a new model architecture that's ONNX-friendly and transfer weights\n",
    "2. Use a hybrid pipeline (ONNX codec + PyTorch transformer)\n",
    "3. Export individual components and reconstruct in the target framework"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}