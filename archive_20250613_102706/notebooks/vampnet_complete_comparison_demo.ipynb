{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VampNet ONNX vs Original Model Comparison Demo\n",
    "\n",
    "This notebook demonstrates the current state of VampNet ONNX export and compares outputs between:\n",
    "1. Original PyTorch VampNet (both coarse and C2F models)\n",
    "2. ONNX exported model (currently only coarse model)\n",
    "\n",
    "## Current Limitations\n",
    "- Only the coarse transformer (4 codebooks) has been exported to ONNX\n",
    "- The coarse-to-fine (C2F) model (10 codebooks) has NOT been exported\n",
    "- ONNX output will be lower quality due to missing fine codebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "from pathlib import Path\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import wavfile\n",
    "import json\n",
    "\n",
    "# VampNet imports\n",
    "from vampnet import mask as pmask\n",
    "from vampnet.interface import Interface\n",
    "\n",
    "# ONNX pipeline imports\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from vampnet_onnx.pipeline import VampNetONNXPipeline\n",
    "from vampnet_onnx.audio_processor import AudioProcessor\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Original VampNet Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original VampNet interface\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize VampNet interface - this loads both coarse and C2F models\n",
    "try:\n",
    "    interface = Interface(device=device)\n",
    "    print(\"\\nOriginal VampNet models loaded:\")\n",
    "    print(f\"- Coarse model: {interface.coarse is not None}\")\n",
    "    print(f\"- C2F model: {interface.c2f is not None}\")\n",
    "    print(f\"- Codec: {interface.codec is not None}\")\n",
    "    print(f\"\\nModel details:\")\n",
    "    print(f\"- Coarse model layers: {len(interface.coarse.net.layers) if hasattr(interface.coarse.net, 'layers') else 'N/A'}\")\n",
    "    print(f\"- C2F model layers: {len(interface.c2f.net.layers) if hasattr(interface.c2f.net, 'layers') else 'N/A'}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading VampNet models: {e}\")\n",
    "    print(\"Make sure you have the model checkpoints in the expected location\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check Available ONNX Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what ONNX models are available\n",
    "onnx_dirs = [\n",
    "    \"../onnx_models\",\n",
    "    \"../onnx_models_optimized\",\n",
    "    \"../onnx_models_quantized\",\n",
    "    \"../onnx_models_test\"\n",
    "]\n",
    "\n",
    "print(\"Available ONNX models:\")\n",
    "for dir_path in onnx_dirs:\n",
    "    if Path(dir_path).exists():\n",
    "        print(f\"\\n{dir_path}:\")\n",
    "        for model_file in Path(dir_path).glob(\"*.onnx\"):\n",
    "            print(f\"  - {model_file.name}\")\n",
    "            \n",
    "# Check for C2F model specifically\n",
    "c2f_models = []\n",
    "for dir_path in onnx_dirs:\n",
    "    if Path(dir_path).exists():\n",
    "        c2f_models.extend(list(Path(dir_path).glob(\"*c2f*.onnx\")))\n",
    "        c2f_models.extend(list(Path(dir_path).glob(\"*fine*.onnx\")))\n",
    "        \n",
    "print(f\"\\nC2F/Fine models found: {len(c2f_models)}\")\n",
    "if c2f_models:\n",
    "    for model in c2f_models:\n",
    "        print(f\"  - {model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load ONNX Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ONNX models\n",
    "codec_path = \"../onnx_models/codec_encoder.onnx\"\n",
    "decoder_path = \"../onnx_models/codec_decoder.onnx\"\n",
    "transformer_path = \"../onnx_models/vampnet_transformer.onnx\"\n",
    "\n",
    "# Check if all required models exist\n",
    "models_exist = {\n",
    "    \"Encoder\": Path(codec_path).exists(),\n",
    "    \"Decoder\": Path(decoder_path).exists(),\n",
    "    \"Transformer (Coarse)\": Path(transformer_path).exists()\n",
    "}\n",
    "\n",
    "print(\"ONNX model status:\")\n",
    "for model_name, exists in models_exist.items():\n",
    "    print(f\"  {model_name}: {'✓ Found' if exists else '✗ Missing'}\")\n",
    "\n",
    "if all(models_exist.values()):\n",
    "    # Initialize ONNX sessions\n",
    "    encoder_session = ort.InferenceSession(codec_path)\n",
    "    decoder_session = ort.InferenceSession(decoder_path)\n",
    "    transformer_session = ort.InferenceSession(transformer_path)\n",
    "    \n",
    "    # Check transformer details\n",
    "    print(\"\\nTransformer ONNX model info:\")\n",
    "    for input in transformer_session.get_inputs():\n",
    "        print(f\"  Input: {input.name}, shape: {input.shape}, dtype: {input.type}\")\n",
    "    for output in transformer_session.get_outputs():\n",
    "        print(f\"  Output: {output.name}, shape: {output.shape}, dtype: {output.type}\")\n",
    "else:\n",
    "    print(\"\\nMissing required ONNX models. Please run export scripts first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Test Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load or generate test audio\n",
    "test_audio_path = \"../assets/test_audio.wav\"  # You can change this to your audio file\n",
    "\n",
    "if Path(test_audio_path).exists():\n",
    "    # Load existing audio\n",
    "    sr, audio_data = wavfile.read(test_audio_path)\n",
    "    audio_data = audio_data.astype(np.float32) / 32768.0  # Normalize to [-1, 1]\n",
    "    if len(audio_data.shape) > 1:\n",
    "        audio_data = audio_data.mean(axis=1)  # Convert to mono\n",
    "    print(f\"Loaded audio: {len(audio_data)/sr:.2f} seconds at {sr} Hz\")\n",
    "else:\n",
    "    # Generate test audio (sine wave)\n",
    "    print(\"Test audio not found. Generating sine wave...\")\n",
    "    sr = 44100\n",
    "    duration = 3.0\n",
    "    t = np.linspace(0, duration, int(sr * duration))\n",
    "    frequency = 440  # A4 note\n",
    "    audio_data = 0.5 * np.sin(2 * np.pi * frequency * t).astype(np.float32)\n",
    "    \n",
    "# Display audio\n",
    "print(f\"Audio shape: {audio_data.shape}\")\n",
    "print(f\"Audio range: [{audio_data.min():.3f}, {audio_data.max():.3f}]\")\n",
    "ipd.display(ipd.Audio(audio_data, rate=sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Process with Original VampNet (Full Pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'interface' in globals():\n",
    "    # Encode audio with original VampNet\n",
    "    print(\"Encoding with original VampNet...\")\n",
    "    \n",
    "    # Convert to torch tensor\n",
    "    audio_tensor = torch.from_numpy(audio_data).unsqueeze(0).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Encode to get latent codes\n",
    "    with torch.no_grad():\n",
    "        z = interface.encode(audio_tensor)\n",
    "        print(f\"Encoded shape: {z.shape}\")\n",
    "        print(f\"Codebooks: {z.shape[1]} (first 4 are coarse, remaining 10 are fine)\")\n",
    "        \n",
    "        # Decode back to audio\n",
    "        reconstructed_audio = interface.decode(z)\n",
    "        reconstructed_audio = reconstructed_audio.squeeze().cpu().numpy()\n",
    "    \n",
    "    print(\"\\nOriginal VampNet reconstruction:\")\n",
    "    ipd.display(ipd.Audio(reconstructed_audio, rate=sr))\n",
    "    \n",
    "    # Save the codes for comparison\n",
    "    original_codes = z.cpu().numpy()\n",
    "    \n",
    "    # Show code statistics\n",
    "    print(\"\\nCode statistics:\")\n",
    "    print(f\"Coarse codes (0-3): unique values = {np.unique(original_codes[0, :4]).shape[0]}\")\n",
    "    print(f\"Fine codes (4-13): unique values = {np.unique(original_codes[0, 4:]).shape[0]}\")\n",
    "else:\n",
    "    print(\"Original VampNet not loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Process with ONNX Pipeline (Coarse Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all(models_exist.values()):\n",
    "    print(\"Processing with ONNX pipeline...\")\n",
    "    \n",
    "    # Initialize ONNX pipeline\n",
    "    pipeline = VampNetONNXPipeline(\n",
    "        encoder_path=codec_path,\n",
    "        decoder_path=decoder_path,\n",
    "        transformer_path=transformer_path\n",
    "    )\n",
    "    \n",
    "    # Process audio\n",
    "    try:\n",
    "        # Encode audio\n",
    "        codes = pipeline.encode_audio(audio_data, sample_rate=sr)\n",
    "        print(f\"ONNX encoded shape: {codes.shape}\")\n",
    "        \n",
    "        # Note: ONNX pipeline currently only uses coarse codes (first 4)\n",
    "        print(\"\\n⚠️ WARNING: ONNX pipeline only processes coarse codes (4 codebooks)\")\n",
    "        print(\"Fine codes (10 codebooks) are padded with zeros for decoding\")\n",
    "        \n",
    "        # Decode back\n",
    "        onnx_reconstructed = pipeline.decode_codes(codes)\n",
    "        \n",
    "        print(\"\\nONNX reconstruction (coarse only):\")\n",
    "        ipd.display(ipd.Audio(onnx_reconstructed, rate=sr))\n",
    "        \n",
    "        # Save ONNX codes for comparison\n",
    "        onnx_codes = codes\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in ONNX pipeline: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"ONNX models not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compare Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison of waveforms\n",
    "if 'reconstructed_audio' in globals() and 'onnx_reconstructed' in globals():\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(12, 8))\n",
    "    \n",
    "    # Original\n",
    "    axes[0].plot(audio_data[:sr//10])  # Show first 0.1 seconds\n",
    "    axes[0].set_title(\"Original Audio\")\n",
    "    axes[0].set_ylabel(\"Amplitude\")\n",
    "    \n",
    "    # PyTorch reconstruction\n",
    "    axes[1].plot(reconstructed_audio[:sr//10])\n",
    "    axes[1].set_title(\"PyTorch VampNet Reconstruction (Full: Coarse + Fine)\")\n",
    "    axes[1].set_ylabel(\"Amplitude\")\n",
    "    \n",
    "    # ONNX reconstruction\n",
    "    axes[2].plot(onnx_reconstructed[:sr//10])\n",
    "    axes[2].set_title(\"ONNX Reconstruction (Coarse Only - Lower Quality)\")\n",
    "    axes[2].set_ylabel(\"Amplitude\")\n",
    "    axes[2].set_xlabel(\"Samples\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate differences\n",
    "    min_len = min(len(reconstructed_audio), len(onnx_reconstructed), len(audio_data))\n",
    "    pytorch_mse = np.mean((audio_data[:min_len] - reconstructed_audio[:min_len])**2)\n",
    "    onnx_mse = np.mean((audio_data[:min_len] - onnx_reconstructed[:min_len])**2)\n",
    "    \n",
    "    print(\"\\nReconstruction Quality (MSE):\")\n",
    "    print(f\"PyTorch (Full): {pytorch_mse:.6f}\")\n",
    "    print(f\"ONNX (Coarse only): {onnx_mse:.6f}\")\n",
    "    print(f\"\\nQuality degradation: {(onnx_mse/pytorch_mse - 1)*100:.1f}% worse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Compare Latent Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'original_codes' in globals() and 'onnx_codes' in globals():\n",
    "    print(\"Latent code comparison:\")\n",
    "    print(f\"Original codes shape: {original_codes.shape}\")\n",
    "    print(f\"ONNX codes shape: {onnx_codes.shape}\")\n",
    "    \n",
    "    # Compare coarse codes\n",
    "    if original_codes.shape[-1] == onnx_codes.shape[-1]:\n",
    "        coarse_match = np.allclose(original_codes[0, :4], onnx_codes[:4], rtol=0.01)\n",
    "        print(f\"\\nCoarse codes (0-3) match: {coarse_match}\")\n",
    "        \n",
    "        if not coarse_match:\n",
    "            diff = np.abs(original_codes[0, :4] - onnx_codes[:4])\n",
    "            print(f\"Max difference in coarse codes: {diff.max()}\")\n",
    "            print(f\"Mean difference in coarse codes: {diff.mean()}\")\n",
    "    \n",
    "    # Check fine codes in ONNX\n",
    "    if onnx_codes.shape[0] > 4:\n",
    "        fine_codes = onnx_codes[4:]\n",
    "        print(f\"\\nFine codes in ONNX: {fine_codes.shape}\")\n",
    "        print(f\"Are fine codes all zeros? {np.all(fine_codes == 0)}\")\n",
    "    else:\n",
    "        print(\"\\n⚠️ ONNX output only contains coarse codes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Check for C2F Model Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for C2F model checkpoint\n",
    "possible_c2f_paths = [\n",
    "    \"../models/c2f.pth\",\n",
    "    \"../models/vampnet_c2f.pth\",\n",
    "    \"../models/coarse_to_fine.pth\",\n",
    "    \"~/.cache/vampnet/c2f.pth\",\n",
    "    \"~/.cache/audiocraft/vampnet/c2f.pth\"\n",
    "]\n",
    "\n",
    "print(\"Searching for C2F model checkpoint...\")\n",
    "c2f_checkpoint = None\n",
    "for path in possible_c2f_paths:\n",
    "    expanded_path = Path(path).expanduser()\n",
    "    if expanded_path.exists():\n",
    "        c2f_checkpoint = expanded_path\n",
    "        print(f\"✓ Found C2F checkpoint: {expanded_path}\")\n",
    "        break\n",
    "    else:\n",
    "        print(f\"✗ Not found: {path}\")\n",
    "\n",
    "if c2f_checkpoint:\n",
    "    # Check if we can load it\n",
    "    try:\n",
    "        checkpoint = torch.load(c2f_checkpoint, map_location='cpu')\n",
    "        print(f\"\\nC2F checkpoint loaded successfully\")\n",
    "        print(f\"Keys in checkpoint: {list(checkpoint.keys())[:5]}...\")\n",
    "        if 'model' in checkpoint:\n",
    "            print(f\"Model state dict keys: {len(checkpoint['model'])}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading C2F checkpoint: {e}\")\n",
    "else:\n",
    "    print(\"\\n⚠️ C2F model checkpoint not found!\")\n",
    "    print(\"This is why only coarse model has been exported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nCurrent State:\")\n",
    "print(\"✓ Coarse transformer exported to ONNX (4 codebooks)\")\n",
    "print(\"✓ Codec (encoder/decoder) exported to ONNX\")\n",
    "print(\"✗ C2F transformer NOT exported (10 codebooks)\")\n",
    "print(\"✗ Complete weight transfer pending (embeddings/classifiers)\")\n",
    "\n",
    "print(\"\\nQuality Impact:\")\n",
    "print(\"- Original VampNet: Uses all 14 codebooks (4 coarse + 10 fine)\")\n",
    "print(\"- ONNX Pipeline: Only uses 4 coarse codebooks\")\n",
    "print(\"- Result: ONNX output is lower quality (missing detail from fine codes)\")\n",
    "\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"1. Export C2F model to ONNX\")\n",
    "print(\"2. Complete weight transfer for embeddings and classifiers\")\n",
    "print(\"3. Implement full two-stage pipeline (coarse → C2F)\")\n",
    "print(\"4. Verify outputs match original quality\")\n",
    "\n",
    "if c2f_checkpoint:\n",
    "    print(f\"\\n✓ Good news: C2F checkpoint found at {c2f_checkpoint}\")\n",
    "    print(\"  We can proceed with exporting the C2F model!\")\n",
    "else:\n",
    "    print(\"\\n⚠️ C2F checkpoint not found - need to locate it first\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}