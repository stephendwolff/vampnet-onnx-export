{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VampNet ONNX Optimization and Quantization\n",
    "\n",
    "This notebook demonstrates how to optimize and quantize VampNet ONNX models for better performance and smaller size."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T18:53:50.543116Z",
     "start_time": "2025-06-06T18:53:40.337528Z"
    }
   },
   "source": "import numpy as np\nimport onnx\nimport onnxruntime as ort\nfrom onnxruntime.quantization import quantize_dynamic, quantize_static, QuantType\nfrom onnxruntime.quantization.calibrate import CalibrationDataReader\nimport os\nimport sys\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\n\n# Add parent directory to path\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath('./'))))\n\n# Import our VampNet ONNX utilities\nfrom vampnet_onnx import (\n    export_all_components,\n    benchmark_model,\n    create_onnx_session\n)\nfrom vampnet_onnx.exporters import export_codec_encoder, export_codec_decoder",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stephen/Documents/Development/MusicHackspace/vampnet-onnx-export/venv/lib/python3.11/site-packages/ignite/handlers/checkpoint.py:16: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n",
      "  from torch.distributed.optim import ZeroRedundancyOptimizer\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Export Models (if not already done)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T18:53:50.711183Z",
     "start_time": "2025-06-06T18:53:50.706567Z"
    }
   },
   "source": "# Export all components if needed\nmodel_dir = \"../onnx_models\"\n\n# Check if we need to export the VampNet codec models\nvampnet_codec_dir = \"../onnx_models/vampnet_codec\"\nif not os.path.exists(os.path.join(vampnet_codec_dir, \"encoder.onnx\")):\n    print(\"Exporting VampNet codec models...\")\n    \n    # Load vampnet interface\n    import vampnet\n    interface = vampnet.interface.Interface(\n        codec_ckpt=\"../models/vampnet/codec.pth\",\n        coarse_ckpt=\"../models/vampnet/coarse.pth\"\n    )\n    codec_model = interface.codec\n    \n    # Export encoder\n    export_codec_encoder(\n        output_path=os.path.join(vampnet_codec_dir, \"encoder.onnx\"),\n        use_vampnet=True,\n        codec_model=codec_model,\n        device='cpu'\n    )\n    \n    # Export decoder\n    export_codec_decoder(\n        output_path=os.path.join(vampnet_codec_dir, \"decoder.onnx\"),\n        use_vampnet=True,\n        codec_model=codec_model,\n        device='cpu'\n    )\nelse:\n    print(f\"VampNet codec models already exported to {vampnet_codec_dir}\")\n\n# Also export other components if needed\nif not os.path.exists(os.path.join(model_dir, \"audio_processor.onnx\")):\n    print(\"\\nExporting other components...\")\n    exported_models = export_all_components(\n        output_dir=model_dir,\n        # Use simplified versions for testing\n        codec_encoder={'use_simplified': True},\n        codec_decoder={'use_simplified': True},\n        transformer={'use_simplified': True}\n    )\nelse:\n    print(f\"\\nOther models already exported to {model_dir}\")\n    \n# List all exported models\nall_models = []\n# Add VampNet codec models\nfor f in Path(vampnet_codec_dir).glob(\"*.onnx\"):\n    all_models.append(f)\n# Add other models\nfor f in Path(model_dir).glob(\"*.onnx\"):\n    if f.parent.name != \"vampnet_codec\":  # Skip duplicates\n        all_models.append(f)\n\nprint(\"\\n\\nAll exported models:\")\nfor model_file in all_models:\n    size_mb = os.path.getsize(model_file) / (1024 * 1024)\n    print(f\"  - {model_file.parent.name}/{model_file.name}: {size_mb:.2f} MB\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VampNet codec models already exported to ../onnx_models/vampnet_codec\n",
      "\n",
      "Other models already exported to ../onnx_models\n",
      "\n",
      "\n",
      "All exported models:\n",
      "  - vampnet_codec/decoder.onnx: 487.08 MB\n",
      "  - vampnet_codec/decoder_opt.onnx: 487.10 MB\n",
      "  - vampnet_codec/encoder.onnx: 87.27 MB\n",
      "  - vampnet_codec/encoder_opt.onnx: 87.63 MB\n",
      "  - onnx_models/transformer.onnx: 38.16 MB\n",
      "  - onnx_models/mask_generator.onnx: 0.01 MB\n",
      "  - onnx_models/codec_decoder_opt.onnx: 100.52 MB\n",
      "  - onnx_models/audio_processor.onnx: 0.01 MB\n",
      "  - onnx_models/codec_decoder.onnx: 100.52 MB\n",
      "  - onnx_models/codec_encoder.onnx: 72.63 MB\n",
      "  - onnx_models/codec_encoder_opt.onnx: 72.61 MB\n",
      "  - onnx_models/transformer_opt.onnx: 38.04 MB\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": "def apply_dynamic_quantization(input_path: str, output_path: str, weight_type=QuantType.QUInt8):\n    \"\"\"\n    Apply dynamic quantization to reduce model size.\n    \"\"\"\n    try:\n        # Fix type inference issues before quantization\n        import onnx\n        import onnx.shape_inference\n        \n        # Load and infer shapes to fix missing type information\n        model = onnx.load(input_path)\n        if len(model.graph.value_info) == 0:\n            print(\"  Fixing missing type information...\")\n            model = onnx.shape_inference.infer_shapes(model)\n            # Save fixed model to temp file\n            temp_path = input_path.replace('.onnx', '_temp.onnx')\n            onnx.save(model, temp_path)\n            input_path = temp_path\n        \n        # For transformer models, we might need to specify extra options\n        extra_options = {}\n        if 'transformer' in input_path.lower():\n            extra_options = {'DefaultTensorType': onnx.TensorProto.FLOAT}\n        \n        # Try quantization\n        quantize_dynamic(\n            model_input=input_path,\n            model_output=output_path,\n            weight_type=weight_type,\n            extra_options=extra_options if extra_options else None\n        )\n        \n        # Clean up temp file if created\n        if 'temp.onnx' in input_path:\n            os.remove(input_path)\n        \n        # Compare sizes\n        original_size = os.path.getsize(input_path.replace('_temp.onnx', '.onnx')) / (1024 * 1024)\n        quantized_size = os.path.getsize(output_path) / (1024 * 1024)\n        reduction = (1 - quantized_size / original_size) * 100\n        \n        print(f\"  Quantized: {original_size:.2f} MB -> {quantized_size:.2f} MB ({reduction:.1f}%)\")\n        return quantized_size\n        \n    except Exception as e:\n        # If it fails, it might be due to unsupported operations\n        print(f\"  Quantization failed: {str(e)[:200]}...\")\n        raise",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T18:53:50.720831Z",
     "start_time": "2025-06-06T18:53:50.717686Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T18:53:54.404824Z",
     "start_time": "2025-06-06T18:53:50.731244Z"
    }
   },
   "source": "def optimize_onnx_model(input_path: str, output_path: str, optimization_level: int = 99):\n    \"\"\"\n    Optimize ONNX model using ONNX Runtime optimizations.\n    \n    Optimization levels:\n    - 0: Disable all optimizations\n    - 1: Basic optimizations\n    - 2: Extended optimizations (portable)\n    - 99: All optimizations (may include hardware-specific)\n    \"\"\"\n    # Create session with optimization\n    sess_options = ort.SessionOptions()\n    \n    # Use EXTENDED level for better portability\n    if optimization_level == 99:\n        sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n        print(\"Warning: Using ALL optimizations (may include hardware-specific optimizations)\")\n    elif optimization_level == 2:\n        sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_EXTENDED\n    elif optimization_level == 1:\n        sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_BASIC\n    else:\n        sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_DISABLE_ALL\n    \n    sess_options.optimized_model_filepath = output_path\n    \n    # Load and optimize\n    _ = ort.InferenceSession(input_path, sess_options)\n    \n    print(f\"Optimized model saved to {output_path}\")\n    \n    # Compare sizes\n    original_size = os.path.getsize(input_path) / (1024 * 1024)\n    optimized_size = os.path.getsize(output_path) / (1024 * 1024)\n    reduction = (1 - optimized_size / original_size) * 100\n    \n    print(f\"Size reduction: {original_size:.2f} MB -> {optimized_size:.2f} MB ({reduction:.1f}%)\")\n    return optimized_size\n\n\n# Optimize each model with EXTENDED level for portability\noptimized_dir = \"../onnx_models_optimized\"\nos.makedirs(optimized_dir, exist_ok=True)\n\noptimization_results = {}\n\n# Use the all_models list we created earlier\nfor model_file in all_models:\n    print(f\"\\nOptimizing {model_file.name}...\")\n    \n    output_path = os.path.join(optimized_dir, model_file.name)\n    # Use level 2 (EXTENDED) for portable optimizations\n    optimized_size = optimize_onnx_model(str(model_file), output_path, optimization_level=2)\n    \n    optimization_results[model_file.name] = {\n        'original': os.path.getsize(model_file) / (1024 * 1024),\n        'optimized': optimized_size\n    }",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimizing decoder.onnx...\n",
      "Optimized model saved to ../onnx_models_optimized/decoder.onnx\n",
      "Size reduction: 487.08 MB -> 487.10 MB (-0.0%)\n",
      "\n",
      "Optimizing decoder_opt.onnx...\n",
      "Optimized model saved to ../onnx_models_optimized/decoder_opt.onnx\n",
      "Size reduction: 487.10 MB -> 487.05 MB (0.0%)\n",
      "\n",
      "Optimizing encoder.onnx...\n",
      "Optimized model saved to ../onnx_models_optimized/encoder.onnx\n",
      "Size reduction: 87.27 MB -> 87.63 MB (-0.4%)\n",
      "\n",
      "Optimizing encoder_opt.onnx...\n",
      "Optimized model saved to ../onnx_models_optimized/encoder_opt.onnx\n",
      "Size reduction: 87.63 MB -> 87.60 MB (0.0%)\n",
      "\n",
      "Optimizing transformer.onnx...\n",
      "Optimized model saved to ../onnx_models_optimized/transformer.onnx\n",
      "Size reduction: 38.16 MB -> 38.04 MB (0.3%)\n",
      "\n",
      "Optimizing mask_generator.onnx...\n",
      "Optimized model saved to ../onnx_models_optimized/mask_generator.onnx\n",
      "Size reduction: 0.01 MB -> 0.00 MB (38.2%)\n",
      "\n",
      "Optimizing codec_decoder_opt.onnx...\n",
      "Optimized model saved to ../onnx_models_optimized/codec_decoder_opt.onnx\n",
      "Size reduction: 100.52 MB -> 100.52 MB (0.0%)\n",
      "\n",
      "Optimizing audio_processor.onnx...\n",
      "Optimized model saved to ../onnx_models_optimized/audio_processor.onnx\n",
      "Size reduction: 0.01 MB -> 0.00 MB (36.2%)\n",
      "\n",
      "Optimizing codec_decoder.onnx...\n",
      "Optimized model saved to ../onnx_models_optimized/codec_decoder.onnx\n",
      "Size reduction: 100.52 MB -> 100.52 MB (0.0%)\n",
      "\n",
      "Optimizing codec_encoder.onnx...\n",
      "Optimized model saved to ../onnx_models_optimized/codec_encoder.onnx\n",
      "Size reduction: 72.63 MB -> 72.61 MB (0.0%)\n",
      "\n",
      "Optimizing codec_encoder_opt.onnx...\n",
      "Optimized model saved to ../onnx_models_optimized/codec_encoder_opt.onnx\n",
      "Size reduction: 72.61 MB -> 72.59 MB (0.0%)\n",
      "\n",
      "Optimizing transformer_opt.onnx...\n",
      "Optimized model saved to ../onnx_models_optimized/transformer_opt.onnx\n",
      "Size reduction: 38.04 MB -> 37.99 MB (0.1%)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": "# Apply dynamic quantization to all models including VampNet codec\nprint(\"\\nApplying dynamic quantization...\")\nprint(\"Note: Warnings about preprocessing and data type inference are normal.\\n\")\n\n# Models to quantize with their configurations\nmodels_to_quantize = [\n    # VampNet codec models\n    (Path(vampnet_codec_dir) / \"encoder.onnx\", \"vampnet_encoder_int8.onnx\", QuantType.QUInt8),\n    (Path(vampnet_codec_dir) / \"decoder.onnx\", \"vampnet_decoder_int8.onnx\", QuantType.QUInt8),\n    # Other models\n    (Path(model_dir) / \"transformer.onnx\", \"transformer_int8.onnx\", QuantType.QUInt8),\n    (Path(model_dir) / \"codec_encoder.onnx\", \"simplified_encoder_int8.onnx\", QuantType.QUInt8),\n    (Path(model_dir) / \"codec_decoder.onnx\", \"simplified_decoder_int8.onnx\", QuantType.QUInt8),\n]\n\nquantized_dir = \"../onnx_models_quantized\"\nos.makedirs(quantized_dir, exist_ok=True)\n\nquantization_results = {}\n\nfor input_path, output_name, weight_type in models_to_quantize:\n    if not input_path.exists():\n        print(f\"Skipping {input_path.name} (not found)\")\n        continue\n        \n    print(f\"\\nQuantizing {input_path.name}...\")\n    output_path = os.path.join(quantized_dir, output_name)\n    \n    try:\n        # First optimize the model\n        optimized_path = str(input_path).replace('.onnx', '_opt.onnx')\n        optimize_onnx_model(str(input_path), optimized_path, optimization_level=2)\n        \n        # Then quantize\n        quantized_size = apply_dynamic_quantization(optimized_path, output_path, weight_type=weight_type)\n        \n        # Clean up temp file\n        if os.path.exists(optimized_path):\n            os.remove(optimized_path)\n            \n        quantization_results[input_path.name] = {\n            'original': os.path.getsize(input_path) / (1024 * 1024),\n            'quantized': quantized_size\n        }\n    except Exception as e:\n        print(f\"  Failed to quantize: {str(e)[:100]}...\")\n\nprint(f\"\\n✓ Quantization complete!\")\nif quantization_results:\n    print(\"\\nQuantized models:\")\n    for model, sizes in quantization_results.items():\n        reduction = (1 - sizes['quantized']/sizes['original']) * 100\n        print(f\"  - {model}: {sizes['original']:.1f}MB → {sizes['quantized']:.1f}MB ({reduction:.1f}% smaller)\")",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T18:54:04.540965Z",
     "start_time": "2025-06-06T18:53:54.415606Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying dynamic quantization...\n",
      "Note: Warnings about preprocessing and data type inference are normal.\n",
      "\n",
      "\n",
      "Quantizing encoder.onnx...\n",
      "Optimized model saved to ../onnx_models/vampnet_codec/encoder_opt.onnx\n",
      "Size reduction: 87.27 MB -> 87.63 MB (-0.4%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/Slice_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 3\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/Slice_1_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 3\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "INFO:root:Quantization parameters for tensor:\"/Pad_1_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.3\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.7\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.11\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.15\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.19\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.23\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.27\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.31\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.35\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.39\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.43\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.47\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.51\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.55\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.59\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.63\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.67\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.71\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.75\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.79\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.83\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.87\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.91\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.95\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.99\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.103\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.107\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.111\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.115\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/encoder/block/block.6/Conv_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/quantizer/quantizers.0/Add_5_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/quantizer/Sub_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/quantizer/quantizers.1/Add_5_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/quantizer/Sub_1_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/quantizer/quantizers.2/Add_5_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/quantizer/Sub_2_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/quantizer/quantizers.3/Add_5_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/quantizer/Sub_3_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/quantizer/quantizers.4/Add_5_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/quantizer/Sub_4_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/quantizer/quantizers.5/Add_5_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/quantizer/Sub_5_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/quantizer/quantizers.6/Add_5_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/quantizer/Sub_6_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/quantizer/quantizers.7/Add_5_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/quantizer/Sub_7_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/quantizer/quantizers.8/Add_5_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/quantizer/Sub_8_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/quantizer/quantizers.9/Add_5_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/quantizer/Sub_9_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/quantizer/quantizers.10/Add_5_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/quantizer/Sub_10_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/quantizer/quantizers.11/Add_5_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/quantizer/Sub_11_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/quantizer/quantizers.12/Add_5_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/quantizer/Sub_12_output_0\" not specified\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Quantized: 87.63 MB -> 22.67 MB (74.1%)\n",
      "\n",
      "Quantizing decoder.onnx...\n",
      "Optimized model saved to ../onnx_models/vampnet_codec/decoder_opt.onnx\n",
      "Size reduction: 487.08 MB -> 487.10 MB (-0.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n",
      "INFO:root:Quantization parameters for tensor:\"/Add_13_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.107\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.111\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.115\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.119\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.123\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.127\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.83\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.87\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.91\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.95\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.99\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.103\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.59\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.63\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.67\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.71\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.75\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.79\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.179\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.183\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.187\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.191\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.195\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.199\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.155\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.159\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.163\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.167\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.171\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.175\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.131\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.135\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.139\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.143\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.147\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.151\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.251\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.255\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.259\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.263\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.267\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.271\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.227\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.231\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.235\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.239\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.243\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.247\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.203\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.207\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.211\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.215\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.219\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.223\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.323\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.327\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.331\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.335\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.339\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.343\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.299\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.303\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.307\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.311\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.315\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.319\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.275\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.279\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.283\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.287\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.291\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.295\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.395\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.399\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.403\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.407\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.411\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.415\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.371\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.375\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.379\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.383\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.387\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.391\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.347\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.351\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.355\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.359\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.363\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.367\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.467\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.471\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.475\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.479\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.483\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.487\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.443\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.447\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.451\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.455\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.459\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.463\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.419\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.423\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.427\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.431\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.435\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"input.439\" not specified\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Quantized: 487.10 MB -> 171.75 MB (64.7%)\n",
      "\n",
      "Quantizing transformer.onnx...\n",
      "Optimized model saved to ../onnx_models/transformer_opt.onnx\n",
      "Size reduction: 38.16 MB -> 38.04 MB (0.3%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n",
      "INFO:root:Quantization parameters for tensor:\"/model/transformer/layers.0/self_attn/Transpose_output_0\" not specified\n",
      "INFO:root:Ignore MatMul due to non constant B: /[/model/transformer/layers.0/self_attn/MatMul_1]\n",
      "INFO:root:Ignore MatMul due to non constant B: /[/model/transformer/layers.0/self_attn/MatMul_2]\n",
      "INFO:root:Quantization parameters for tensor:\"/model/transformer/layers.0/self_attn/Reshape_9_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/model/transformer/layers.0/norm1/Add_1_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/model/transformer/layers.0/Relu_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/model/transformer/layers.1/self_attn/Transpose_output_0\" not specified\n",
      "INFO:root:Ignore MatMul due to non constant B: /[/model/transformer/layers.1/self_attn/MatMul_1]\n",
      "INFO:root:Ignore MatMul due to non constant B: /[/model/transformer/layers.1/self_attn/MatMul_2]\n",
      "INFO:root:Quantization parameters for tensor:\"/model/transformer/layers.1/self_attn/Reshape_9_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/model/transformer/layers.1/norm1/Add_1_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/model/transformer/layers.1/Relu_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/model/transformer/layers.2/self_attn/Transpose_output_0\" not specified\n",
      "INFO:root:Ignore MatMul due to non constant B: /[/model/transformer/layers.2/self_attn/MatMul_1]\n",
      "INFO:root:Ignore MatMul due to non constant B: /[/model/transformer/layers.2/self_attn/MatMul_2]\n",
      "INFO:root:Quantization parameters for tensor:\"/model/transformer/layers.2/self_attn/Reshape_9_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/model/transformer/layers.2/norm1/Add_1_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/model/transformer/layers.2/Relu_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/model/transformer/layers.3/self_attn/Transpose_output_0\" not specified\n",
      "INFO:root:Ignore MatMul due to non constant B: /[/model/transformer/layers.3/self_attn/MatMul_1]\n",
      "INFO:root:Ignore MatMul due to non constant B: /[/model/transformer/layers.3/self_attn/MatMul_2]\n",
      "INFO:root:Quantization parameters for tensor:\"/model/transformer/layers.3/self_attn/Reshape_9_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/model/transformer/layers.3/norm1/Add_1_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/model/transformer/layers.3/Relu_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/model/transformer/layers.4/self_attn/Transpose_output_0\" not specified\n",
      "INFO:root:Ignore MatMul due to non constant B: /[/model/transformer/layers.4/self_attn/MatMul_1]\n",
      "INFO:root:Ignore MatMul due to non constant B: /[/model/transformer/layers.4/self_attn/MatMul_2]\n",
      "INFO:root:Quantization parameters for tensor:\"/model/transformer/layers.4/self_attn/Reshape_9_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/model/transformer/layers.4/norm1/Add_1_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/model/transformer/layers.4/Relu_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/model/transformer/layers.5/self_attn/Transpose_output_0\" not specified\n",
      "INFO:root:Ignore MatMul due to non constant B: /[/model/transformer/layers.5/self_attn/MatMul_1]\n",
      "INFO:root:Ignore MatMul due to non constant B: /[/model/transformer/layers.5/self_attn/MatMul_2]\n",
      "INFO:root:Quantization parameters for tensor:\"/model/transformer/layers.5/self_attn/Reshape_9_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/model/transformer/layers.5/norm1/Add_1_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/model/transformer/layers.5/Relu_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/model/transformer/layers.5/norm2/Add_1_output_0\" not specified\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Quantized: 38.04 MB -> 17.07 MB (55.1%)\n",
      "\n",
      "Quantizing codec_encoder.onnx...\n",
      "Optimized model saved to ../onnx_models/codec_encoder_opt.onnx\n",
      "Size reduction: 72.63 MB -> 72.61 MB (0.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n",
      "INFO:root:Quantization parameters for tensor:\"/encoder/encoder.3/Relu_output_0\" not specified\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Quantized: 72.61 MB -> 41.11 MB (43.4%)\n",
      "\n",
      "Quantizing codec_decoder.onnx...\n",
      "Optimized model saved to ../onnx_models/codec_decoder_opt.onnx\n",
      "Size reduction: 100.52 MB -> 100.52 MB (0.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Quantized: 100.52 MB -> 79.53 MB (20.9%)\n",
      "\n",
      "✓ Quantization complete!\n",
      "\n",
      "Quantized models:\n",
      "  - encoder.onnx: 87.3MB → 22.7MB (74.0% smaller)\n",
      "  - decoder.onnx: 487.1MB → 171.8MB (64.7% smaller)\n",
      "  - transformer.onnx: 38.2MB → 17.1MB (55.3% smaller)\n",
      "  - codec_encoder.onnx: 72.6MB → 41.1MB (43.4% smaller)\n",
      "  - codec_decoder.onnx: 100.5MB → 79.5MB (20.9% smaller)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T18:54:04.552877Z",
     "start_time": "2025-06-06T18:54:04.551425Z"
    }
   },
   "source": "# This cell has been moved to the previous cell",
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Static Quantization (Advanced)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T18:54:06.124860Z",
     "start_time": "2025-06-06T18:54:04.566989Z"
    }
   },
   "source": "class VampNetCalibrationDataReader(CalibrationDataReader):\n    \"\"\"\n    Calibration data reader for static quantization.\n    \"\"\"\n    def __init__(self, model_name: str, n_samples: int = 10):\n        self.model_name = model_name\n        self.n_samples = n_samples\n        self.data_iter = self._generate_calibration_data()\n        \n    def _generate_calibration_data(self):\n        \"\"\"Generate calibration data based on model type.\"\"\"\n        for i in range(self.n_samples):\n            if 'audio_processor' in self.model_name:\n                # Generate random audio\n                yield {'audio': np.random.randn(1, 2, 44100).astype(np.float32)}\n                \n            elif 'codec_encoder' in self.model_name:\n                # Generate preprocessed audio\n                yield {'audio': np.random.randn(1, 1, 44100).astype(np.float32)}\n                \n            elif 'mask_generator' in self.model_name:\n                # Generate token codes\n                yield {'codes': np.random.randint(0, 1024, (1, 14, 100), dtype=np.int64)}\n                \n            elif 'transformer' in self.model_name:\n                # Generate codes and mask\n                yield {\n                    'codes': np.random.randint(0, 1024, (1, 4, 100), dtype=np.int64),\n                    'mask': np.random.randint(0, 2, (1, 4, 100), dtype=np.int64)\n                }\n                \n            elif 'codec_decoder' in self.model_name:\n                # Generate token codes\n                yield {'codes': np.random.randint(0, 1024, (1, 14, 100), dtype=np.int64)}\n                \n    def get_next(self):\n        return next(self.data_iter, None)\n\n\ndef apply_static_quantization(input_path: str, output_path: str, model_name: str):\n    \"\"\"\n    Apply static quantization using calibration data.\n    \"\"\"\n    # Create calibration reader\n    calibration_reader = VampNetCalibrationDataReader(model_name)\n    \n    # Apply static quantization without optimize_model parameter\n    quantize_static(\n        model_input=input_path,\n        model_output=output_path,\n        calibration_data_reader=calibration_reader,\n        quant_format=QuantType.QInt8,\n        weight_type=QuantType.QInt8\n    )\n    \n    # Compare sizes\n    original_size = os.path.getsize(input_path) / (1024 * 1024)\n    quantized_size = os.path.getsize(output_path) / (1024 * 1024)\n    reduction = (1 - quantized_size / original_size) * 100\n    \n    print(f\"Static quantized: {original_size:.2f} MB -> {quantized_size:.2f} MB ({reduction:.1f}%)\")\n    return quantized_size\n\n\n# Apply static quantization to transformer (most compute-intensive)\nstatic_quantized_dir = \"onnx_models_static_quantized\"\nos.makedirs(static_quantized_dir, exist_ok=True)\n\ntransformer_path = os.path.join(optimized_dir, \"transformer.onnx\")\nif os.path.exists(transformer_path):\n    print(\"\\nApplying static quantization to transformer...\")\n    output_path = os.path.join(static_quantized_dir, \"transformer_static_int8.onnx\")\n    \n    try:\n        static_size = apply_static_quantization(transformer_path, output_path, \"transformer\")\n    except Exception as e:\n        print(f\"Static quantization failed: {e}\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying static quantization to transformer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n",
      "WARNING:root:Please use QuantFormat.QDQ for activation type QInt8 and weight type QInt8. Or it will lead to bad performance on x64.\n",
      "INFO:root:Created a copy of bias input 'model.transformer.layers.0.self_attn.out_proj.bias' called 'model.transformer.layers.0.self_attn.out_proj.bias0'\n",
      "INFO:root:Created a copy of bias input 'model.transformer.layers.0.self_attn.out_proj.bias' called 'model.transformer.layers.0.self_attn.out_proj.bias1'\n",
      "INFO:root:Created a copy of bias input 'model.transformer.layers.0.self_attn.out_proj.bias' called 'model.transformer.layers.0.self_attn.out_proj.bias2'\n",
      "INFO:root:Created a copy of bias input 'model.transformer.layers.0.self_attn.out_proj.bias' called 'model.transformer.layers.0.self_attn.out_proj.bias3'\n",
      "INFO:root:Created a copy of bias input 'model.transformer.layers.0.self_attn.out_proj.bias' called 'model.transformer.layers.0.self_attn.out_proj.bias4'\n",
      "INFO:root:Created a copy of bias input 'model.transformer.layers.0.self_attn.out_proj.bias' called 'model.transformer.layers.0.self_attn.out_proj.bias5'\n",
      "INFO:root:Created a copy of bias input 'model.transformer.layers.0.self_attn.out_proj.bias' called 'model.transformer.layers.0.self_attn.out_proj.bias6'\n",
      "INFO:root:Created a copy of bias input 'model.transformer.layers.0.self_attn.out_proj.bias' called 'model.transformer.layers.0.self_attn.out_proj.bias7'\n",
      "INFO:root:Created a copy of bias input 'model.transformer.layers.0.self_attn.out_proj.bias' called 'model.transformer.layers.0.self_attn.out_proj.bias8'\n",
      "INFO:root:Created a copy of bias input 'model.transformer.layers.0.self_attn.out_proj.bias' called 'model.transformer.layers.0.self_attn.out_proj.bias9'\n",
      "INFO:root:Created a copy of bias input 'model.transformer.layers.0.self_attn.out_proj.bias' called 'model.transformer.layers.0.self_attn.out_proj.bias10'\n",
      "WARNING:root:Please consider pre-processing before quantization. See https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Static quantized: 38.04 MB -> 17.23 MB (54.7%)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T18:55:19.401057Z",
     "start_time": "2025-06-06T18:54:06.135164Z"
    }
   },
   "source": [
    "def compare_model_performance(model_paths: dict, input_data: dict, n_runs: int = 100):\n",
    "    \"\"\"\n",
    "    Compare performance of different model variants.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for name, path in model_paths.items():\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"Skipping {name}: file not found\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nBenchmarking {name}...\")\n",
    "        \n",
    "        try:\n",
    "            # Create session\n",
    "            session = create_onnx_session(path)\n",
    "            \n",
    "            # Benchmark\n",
    "            stats = benchmark_model(session, input_data, n_runs=n_runs, warmup_runs=10)\n",
    "            \n",
    "            # Add model size\n",
    "            stats['size_mb'] = os.path.getsize(path) / (1024 * 1024)\n",
    "            \n",
    "            results[name] = stats\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to benchmark {name}: {e}\")\n",
    "            \n",
    "    return results\n",
    "\n",
    "\n",
    "# Compare transformer variants\n",
    "transformer_variants = {\n",
    "    'original': os.path.join(model_dir, 'transformer.onnx'),\n",
    "    'optimized': os.path.join(optimized_dir, 'transformer.onnx'),\n",
    "    'dynamic_int8': os.path.join(quantized_dir, 'transformer_int8.onnx'),\n",
    "    'static_int8': os.path.join(static_quantized_dir, 'transformer_static_int8.onnx')\n",
    "}\n",
    "\n",
    "# Test data\n",
    "test_input = {\n",
    "    'codes': np.random.randint(0, 1024, (1, 4, 100), dtype=np.int64),\n",
    "    'mask': np.random.randint(0, 2, (1, 4, 100), dtype=np.int64)\n",
    "}\n",
    "\n",
    "# Run comparison\n",
    "perf_results = compare_model_performance(transformer_variants, test_input)\n",
    "\n",
    "# Visualize results\n",
    "if perf_results:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Inference time\n",
    "    names = list(perf_results.keys())\n",
    "    mean_times = [perf_results[n]['mean_ms'] for n in names]\n",
    "    \n",
    "    ax1.bar(names, mean_times)\n",
    "    ax1.set_ylabel('Mean Inference Time (ms)')\n",
    "    ax1.set_title('Inference Speed Comparison')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Model size\n",
    "    sizes = [perf_results[n]['size_mb'] for n in names]\n",
    "    \n",
    "    ax2.bar(names, sizes)\n",
    "    ax2.set_ylabel('Model Size (MB)')\n",
    "    ax2.set_title('Model Size Comparison')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[0;93m2025-06-06 19:54:06.172374 [W:onnxruntime:, helper.cc:89 IsInputSupported] CoreML does not support shapes with dimension values of 0. Input:/model/transformer/layers.0/self_attn/Slice_2_output_0, shape: {0}\u001B[m\n",
      "\u001B[0;93m2025-06-06 19:54:06.172418 [W:onnxruntime:, helper.cc:89 IsInputSupported] CoreML does not support shapes with dimension values of 0. Input:/model/transformer/layers.1/self_attn/Slice_2_output_0, shape: {0}\u001B[m\n",
      "\u001B[0;93m2025-06-06 19:54:06.172440 [W:onnxruntime:, helper.cc:89 IsInputSupported] CoreML does not support shapes with dimension values of 0. Input:/model/transformer/layers.2/self_attn/Slice_2_output_0, shape: {0}\u001B[m\n",
      "\u001B[0;93m2025-06-06 19:54:06.172461 [W:onnxruntime:, helper.cc:89 IsInputSupported] CoreML does not support shapes with dimension values of 0. Input:/model/transformer/layers.3/self_attn/Slice_2_output_0, shape: {0}\u001B[m\n",
      "\u001B[0;93m2025-06-06 19:54:06.172484 [W:onnxruntime:, helper.cc:89 IsInputSupported] CoreML does not support shapes with dimension values of 0. Input:/model/transformer/layers.4/self_attn/Slice_2_output_0, shape: {0}\u001B[m\n",
      "\u001B[0;93m2025-06-06 19:54:06.172508 [W:onnxruntime:, helper.cc:89 IsInputSupported] CoreML does not support shapes with dimension values of 0. Input:/model/transformer/layers.5/self_attn/Slice_2_output_0, shape: {0}\u001B[m\n",
      "\u001B[0;93m2025-06-06 19:54:06.172825 [W:onnxruntime:, coreml_execution_provider.cc:113 GetCapability] CoreMLExecutionProvider::GetCapability, number of partitions supported by CoreML: 75 number of nodes in the graph: 842 number of nodes supported by CoreML: 308\u001B[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Benchmarking original...\n",
      "Created ONNX Runtime session with providers: ['CoreMLExecutionProvider', 'CPUExecutionProvider']\n",
      "Benchmarking model (100 runs after 10 warmup)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Context leak detected, msgtracer returned -1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark results:\n",
      "  - mean_ms: 115.90\n",
      "  - std_ms: 6.02\n",
      "  - min_ms: 104.44\n",
      "  - max_ms: 131.26\n",
      "  - median_ms: 115.42\n",
      "  - p95_ms: 125.39\n",
      "  - p99_ms: 131.16\n",
      "\n",
      "Benchmarking optimized...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[0;93m2025-06-06 19:54:24.453629 [W:onnxruntime:, helper.cc:89 IsInputSupported] CoreML does not support shapes with dimension values of 0. Input:/model/transformer/layers.0/self_attn/Slice_2_output_0, shape: {0}\u001B[m\n",
      "\u001B[0;93m2025-06-06 19:54:24.453668 [W:onnxruntime:, helper.cc:89 IsInputSupported] CoreML does not support shapes with dimension values of 0. Input:/model/transformer/layers.1/self_attn/Slice_2_output_0, shape: {0}\u001B[m\n",
      "\u001B[0;93m2025-06-06 19:54:24.453686 [W:onnxruntime:, helper.cc:89 IsInputSupported] CoreML does not support shapes with dimension values of 0. Input:/model/transformer/layers.2/self_attn/Slice_2_output_0, shape: {0}\u001B[m\n",
      "\u001B[0;93m2025-06-06 19:54:24.453704 [W:onnxruntime:, helper.cc:89 IsInputSupported] CoreML does not support shapes with dimension values of 0. Input:/model/transformer/layers.3/self_attn/Slice_2_output_0, shape: {0}\u001B[m\n",
      "\u001B[0;93m2025-06-06 19:54:24.453720 [W:onnxruntime:, helper.cc:89 IsInputSupported] CoreML does not support shapes with dimension values of 0. Input:/model/transformer/layers.4/self_attn/Slice_2_output_0, shape: {0}\u001B[m\n",
      "\u001B[0;93m2025-06-06 19:54:24.453736 [W:onnxruntime:, helper.cc:89 IsInputSupported] CoreML does not support shapes with dimension values of 0. Input:/model/transformer/layers.5/self_attn/Slice_2_output_0, shape: {0}\u001B[m\n",
      "\u001B[0;93m2025-06-06 19:54:24.453982 [W:onnxruntime:, coreml_execution_provider.cc:113 GetCapability] CoreMLExecutionProvider::GetCapability, number of partitions supported by CoreML: 80 number of nodes in the graph: 740 number of nodes supported by CoreML: 194\u001B[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created ONNX Runtime session with providers: ['CoreMLExecutionProvider', 'CPUExecutionProvider']\n",
      "Benchmarking model (100 runs after 10 warmup)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Context leak detected, msgtracer returned -1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark results:\n",
      "  - mean_ms: 88.32\n",
      "  - std_ms: 7.97\n",
      "  - min_ms: 75.03\n",
      "  - max_ms: 120.54\n",
      "  - median_ms: 85.97\n",
      "  - p95_ms: 101.26\n",
      "  - p99_ms: 103.25\n",
      "\n",
      "Benchmarking dynamic_int8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[0;93m2025-06-06 19:54:40.097914 [W:onnxruntime:, helper.cc:89 IsInputSupported] CoreML does not support shapes with dimension values of 0. Input:/model/transformer/layers.0/self_attn/Slice_2_output_0, shape: {0}\u001B[m\n",
      "\u001B[0;93m2025-06-06 19:54:40.097961 [W:onnxruntime:, helper.cc:89 IsInputSupported] CoreML does not support shapes with dimension values of 0. Input:/model/transformer/layers.1/self_attn/Slice_2_output_0, shape: {0}\u001B[m\n",
      "\u001B[0;93m2025-06-06 19:54:40.097983 [W:onnxruntime:, helper.cc:89 IsInputSupported] CoreML does not support shapes with dimension values of 0. Input:/model/transformer/layers.2/self_attn/Slice_2_output_0, shape: {0}\u001B[m\n",
      "\u001B[0;93m2025-06-06 19:54:40.098003 [W:onnxruntime:, helper.cc:89 IsInputSupported] CoreML does not support shapes with dimension values of 0. Input:/model/transformer/layers.3/self_attn/Slice_2_output_0, shape: {0}\u001B[m\n",
      "\u001B[0;93m2025-06-06 19:54:40.098022 [W:onnxruntime:, helper.cc:89 IsInputSupported] CoreML does not support shapes with dimension values of 0. Input:/model/transformer/layers.4/self_attn/Slice_2_output_0, shape: {0}\u001B[m\n",
      "\u001B[0;93m2025-06-06 19:54:40.098041 [W:onnxruntime:, helper.cc:89 IsInputSupported] CoreML does not support shapes with dimension values of 0. Input:/model/transformer/layers.5/self_attn/Slice_2_output_0, shape: {0}\u001B[m\n",
      "\u001B[0;93m2025-06-06 19:54:40.098308 [W:onnxruntime:, coreml_execution_provider.cc:113 GetCapability] CoreMLExecutionProvider::GetCapability, number of partitions supported by CoreML: 80 number of nodes in the graph: 848 number of nodes supported by CoreML: 243\u001B[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created ONNX Runtime session with providers: ['CoreMLExecutionProvider', 'CPUExecutionProvider']\n",
      "Benchmarking model (100 runs after 10 warmup)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Context leak detected, msgtracer returned -1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark results:\n",
      "  - mean_ms: 116.91\n",
      "  - std_ms: 5.89\n",
      "  - min_ms: 104.19\n",
      "  - max_ms: 145.19\n",
      "  - median_ms: 115.92\n",
      "  - p95_ms: 129.57\n",
      "  - p99_ms: 134.04\n",
      "\n",
      "Benchmarking static_int8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[0;93m2025-06-06 19:54:59.197907 [W:onnxruntime:, helper.cc:89 IsInputSupported] CoreML does not support shapes with dimension values of 0. Input:/model/transformer/layers.0/self_attn/Slice_2_output_0, shape: {0}\u001B[m\n",
      "\u001B[0;93m2025-06-06 19:54:59.197954 [W:onnxruntime:, helper.cc:89 IsInputSupported] CoreML does not support shapes with dimension values of 0. Input:/model/transformer/layers.1/self_attn/Slice_2_output_0, shape: {0}\u001B[m\n",
      "\u001B[0;93m2025-06-06 19:54:59.197978 [W:onnxruntime:, helper.cc:89 IsInputSupported] CoreML does not support shapes with dimension values of 0. Input:/model/transformer/layers.2/self_attn/Slice_2_output_0, shape: {0}\u001B[m\n",
      "\u001B[0;93m2025-06-06 19:54:59.197999 [W:onnxruntime:, helper.cc:89 IsInputSupported] CoreML does not support shapes with dimension values of 0. Input:/model/transformer/layers.3/self_attn/Slice_2_output_0, shape: {0}\u001B[m\n",
      "\u001B[0;93m2025-06-06 19:54:59.198020 [W:onnxruntime:, helper.cc:89 IsInputSupported] CoreML does not support shapes with dimension values of 0. Input:/model/transformer/layers.4/self_attn/Slice_2_output_0, shape: {0}\u001B[m\n",
      "\u001B[0;93m2025-06-06 19:54:59.198039 [W:onnxruntime:, helper.cc:89 IsInputSupported] CoreML does not support shapes with dimension values of 0. Input:/model/transformer/layers.5/self_attn/Slice_2_output_0, shape: {0}\u001B[m\n",
      "\u001B[0;93m2025-06-06 19:54:59.198373 [W:onnxruntime:, coreml_execution_provider.cc:113 GetCapability] CoreMLExecutionProvider::GetCapability, number of partitions supported by CoreML: 106 number of nodes in the graph: 1357 number of nodes supported by CoreML: 182\u001B[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created ONNX Runtime session with providers: ['CoreMLExecutionProvider', 'CPUExecutionProvider']\n",
      "Benchmarking model (100 runs after 10 warmup)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Context leak detected, msgtracer returned -1\n",
      "Context leak detected, msgtracer returned -1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark results:\n",
      "  - mean_ms: 106.21\n",
      "  - std_ms: 4.01\n",
      "  - min_ms: 97.47\n",
      "  - max_ms: 116.35\n",
      "  - median_ms: 105.51\n",
      "  - p95_ms: 113.80\n",
      "  - p99_ms: 115.97\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Pruning (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T18:55:20.550928Z",
     "start_time": "2025-06-06T18:55:19.449843Z"
    }
   },
   "source": [
    "def analyze_model_sparsity(model_path: str):\n",
    "    \"\"\"\n",
    "    Analyze weight sparsity in the model.\n",
    "    \"\"\"\n",
    "    model = onnx.load(model_path)\n",
    "    \n",
    "    total_params = 0\n",
    "    zero_params = 0\n",
    "    \n",
    "    for initializer in model.graph.initializer:\n",
    "        # Convert to numpy array\n",
    "        if initializer.data_type == onnx.TensorProto.FLOAT:\n",
    "            weights = np.frombuffer(initializer.raw_data, dtype=np.float32)\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "        total_params += weights.size\n",
    "        zero_params += np.sum(np.abs(weights) < 1e-6)\n",
    "        \n",
    "    sparsity = zero_params / total_params * 100 if total_params > 0 else 0\n",
    "    \n",
    "    print(f\"Model sparsity: {sparsity:.1f}%\")\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Zero parameters: {zero_params:,}\")\n",
    "    \n",
    "    return sparsity\n",
    "\n",
    "\n",
    "# Analyze sparsity of models\n",
    "print(\"\\nAnalyzing model sparsity...\")\n",
    "for model_file in Path(optimized_dir).glob(\"*.onnx\"):\n",
    "    print(f\"\\n{model_file.name}:\")\n",
    "    analyze_model_sparsity(str(model_file))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing model sparsity...\n",
      "\n",
      "decoder.onnx:\n",
      "Model sparsity: 0.0%\n",
      "Total parameters: 127,612,627\n",
      "Zero parameters: 47,569\n",
      "\n",
      "transformer.onnx:\n",
      "Model sparsity: 0.0%\n",
      "Total parameters: 9,911,297\n",
      "Zero parameters: 2,448\n",
      "\n",
      "mask_generator.onnx:\n",
      "Model sparsity: 0.0%\n",
      "Total parameters: 0\n",
      "Zero parameters: 0\n",
      "\n",
      "decoder_opt.onnx:\n",
      "Model sparsity: 0.0%\n",
      "Total parameters: 127,612,627\n",
      "Zero parameters: 47,569\n",
      "\n",
      "codec_decoder_opt.onnx:\n",
      "Model sparsity: 0.0%\n",
      "Total parameters: 26,348,033\n",
      "Zero parameters: 981\n",
      "\n",
      "audio_processor.onnx:\n",
      "Model sparsity: 25.0%\n",
      "Total parameters: 4\n",
      "Zero parameters: 1\n",
      "\n",
      "codec_decoder.onnx:\n",
      "Model sparsity: 0.0%\n",
      "Total parameters: 26,348,033\n",
      "Zero parameters: 981\n",
      "\n",
      "encoder.onnx:\n",
      "Model sparsity: 0.0%\n",
      "Total parameters: 22,908,146\n",
      "Zero parameters: 2,868\n",
      "\n",
      "codec_encoder.onnx:\n",
      "Model sparsity: 0.0%\n",
      "Total parameters: 19,015,170\n",
      "Zero parameters: 477\n",
      "\n",
      "codec_encoder_opt.onnx:\n",
      "Model sparsity: 0.0%\n",
      "Total parameters: 19,015,170\n",
      "Zero parameters: 477\n",
      "\n",
      "encoder_opt.onnx:\n",
      "Model sparsity: 0.0%\n",
      "Total parameters: 22,908,146\n",
      "Zero parameters: 2,868\n",
      "\n",
      "transformer_opt.onnx:\n",
      "Model sparsity: 0.0%\n",
      "Total parameters: 9,911,297\n",
      "Zero parameters: 2,448\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Optimization Summary"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T18:55:20.563506Z",
     "start_time": "2025-06-06T18:55:20.560238Z"
    }
   },
   "source": [
    "# Create optimization summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"OPTIMIZATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for model_name in optimization_results:\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    \n",
    "    original_size = optimization_results[model_name]['original']\n",
    "    optimized_size = optimization_results[model_name]['optimized']\n",
    "    \n",
    "    print(f\"  Original: {original_size:.2f} MB\")\n",
    "    print(f\"  Optimized: {optimized_size:.2f} MB ({(1-optimized_size/original_size)*100:.1f}% reduction)\")\n",
    "    \n",
    "    # Check if quantized version exists\n",
    "    if model_name in quantization_results:\n",
    "        quantized_size = quantization_results[model_name]['quantized']\n",
    "        print(f\"  Quantized: {quantized_size:.2f} MB ({(1-quantized_size/original_size)*100:.1f}% reduction)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Recommendations\n",
    "print(\"\\nRECOMMENDATIONS:\")\n",
    "print(\"1. Use optimized models for production deployment\")\n",
    "print(\"2. Use quantized models for edge devices or when model size is critical\")\n",
    "print(\"3. Static quantization provides best performance but requires calibration data\")\n",
    "print(\"4. Test accuracy degradation after quantization for your use case\")\n",
    "print(\"5. Consider model-specific optimizations based on deployment target\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "OPTIMIZATION SUMMARY\n",
      "============================================================\n",
      "\n",
      "decoder.onnx:\n",
      "  Original: 487.08 MB\n",
      "  Optimized: 487.10 MB (-0.0% reduction)\n",
      "  Quantized: 171.75 MB (64.7% reduction)\n",
      "\n",
      "decoder_opt.onnx:\n",
      "  Original: 487.10 MB\n",
      "  Optimized: 487.05 MB (0.0% reduction)\n",
      "\n",
      "encoder.onnx:\n",
      "  Original: 87.27 MB\n",
      "  Optimized: 87.63 MB (-0.4% reduction)\n",
      "  Quantized: 22.67 MB (74.0% reduction)\n",
      "\n",
      "encoder_opt.onnx:\n",
      "  Original: 87.63 MB\n",
      "  Optimized: 87.60 MB (0.0% reduction)\n",
      "\n",
      "transformer.onnx:\n",
      "  Original: 38.16 MB\n",
      "  Optimized: 38.04 MB (0.3% reduction)\n",
      "  Quantized: 17.07 MB (55.3% reduction)\n",
      "\n",
      "mask_generator.onnx:\n",
      "  Original: 0.01 MB\n",
      "  Optimized: 0.00 MB (38.2% reduction)\n",
      "\n",
      "codec_decoder_opt.onnx:\n",
      "  Original: 100.52 MB\n",
      "  Optimized: 100.52 MB (0.0% reduction)\n",
      "\n",
      "audio_processor.onnx:\n",
      "  Original: 0.01 MB\n",
      "  Optimized: 0.00 MB (36.2% reduction)\n",
      "\n",
      "codec_decoder.onnx:\n",
      "  Original: 100.52 MB\n",
      "  Optimized: 100.52 MB (0.0% reduction)\n",
      "  Quantized: 79.53 MB (20.9% reduction)\n",
      "\n",
      "codec_encoder.onnx:\n",
      "  Original: 72.63 MB\n",
      "  Optimized: 72.61 MB (0.0% reduction)\n",
      "  Quantized: 41.11 MB (43.4% reduction)\n",
      "\n",
      "codec_encoder_opt.onnx:\n",
      "  Original: 72.61 MB\n",
      "  Optimized: 72.59 MB (0.0% reduction)\n",
      "\n",
      "transformer_opt.onnx:\n",
      "  Original: 38.04 MB\n",
      "  Optimized: 37.99 MB (0.1% reduction)\n",
      "\n",
      "============================================================\n",
      "\n",
      "RECOMMENDATIONS:\n",
      "1. Use optimized models for production deployment\n",
      "2. Use quantized models for edge devices or when model size is critical\n",
      "3. Static quantization provides best performance but requires calibration data\n",
      "4. Test accuracy degradation after quantization for your use case\n",
      "5. Consider model-specific optimizations based on deployment target\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Optimization Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T18:55:22.889596Z",
     "start_time": "2025-06-06T18:55:20.577271Z"
    }
   },
   "source": [
    "def create_optimized_pipeline(base_model_dir: str, output_dir: str, optimization_config: dict):\n",
    "    \"\"\"\n",
    "    Create a complete optimized pipeline with specified optimization settings.\n",
    "    \n",
    "    Args:\n",
    "        base_model_dir: Directory with original ONNX models\n",
    "        output_dir: Output directory for optimized models\n",
    "        optimization_config: Dict with optimization settings per model\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for model_name, config in optimization_config.items():\n",
    "        input_path = os.path.join(base_model_dir, f\"{model_name}.onnx\")\n",
    "        if not os.path.exists(input_path):\n",
    "            print(f\"Skipping {model_name}: not found\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nOptimizing {model_name}...\")\n",
    "        \n",
    "        # Step 1: Graph optimization\n",
    "        if config.get('optimize_graph', True):\n",
    "            temp_path = os.path.join(output_dir, f\"{model_name}_opt.onnx\")\n",
    "            optimize_onnx_model(input_path, temp_path)\n",
    "            input_path = temp_path\n",
    "            \n",
    "        # Step 2: Quantization\n",
    "        if config.get('quantize', False):\n",
    "            output_path = os.path.join(output_dir, f\"{model_name}_final.onnx\")\n",
    "            quantize_type = config.get('quantize_type', 'dynamic')\n",
    "            \n",
    "            if quantize_type == 'dynamic':\n",
    "                apply_dynamic_quantization(input_path, output_path)\n",
    "            elif quantize_type == 'static':\n",
    "                apply_static_quantization(input_path, output_path, model_name)\n",
    "        else:\n",
    "            output_path = os.path.join(output_dir, f\"{model_name}_final.onnx\")\n",
    "            import shutil\n",
    "            shutil.copy(input_path, output_path)\n",
    "            \n",
    "        # Clean up temp files\n",
    "        if config.get('optimize_graph', True) and os.path.exists(temp_path):\n",
    "            os.remove(temp_path)\n",
    "            \n",
    "    print(f\"\\nOptimized pipeline created in {output_dir}\")\n",
    "\n",
    "\n",
    "# Example configuration for production deployment\n",
    "production_config = {\n",
    "    'audio_processor': {\n",
    "        'optimize_graph': True,\n",
    "        'quantize': False  # Keep float for audio processing\n",
    "    },\n",
    "    'codec_encoder': {\n",
    "        'optimize_graph': True,\n",
    "        'quantize': True,\n",
    "        'quantize_type': 'dynamic'\n",
    "    },\n",
    "    'mask_generator': {\n",
    "        'optimize_graph': True,\n",
    "        'quantize': False  # Simple operation, minimal benefit\n",
    "    },\n",
    "    'transformer': {\n",
    "        'optimize_graph': True,\n",
    "        'quantize': True,\n",
    "        'quantize_type': 'dynamic'  # Use 'static' for best performance\n",
    "    },\n",
    "    'codec_decoder': {\n",
    "        'optimize_graph': True,\n",
    "        'quantize': True,\n",
    "        'quantize_type': 'dynamic'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create production pipeline\n",
    "create_optimized_pipeline(model_dir, \"onnx_models_production\", production_config)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimizing audio_processor...\n",
      "Warning: Using ALL optimizations (may include hardware-specific optimizations)\n",
      "Optimized model saved to onnx_models_production/audio_processor_opt.onnx\n",
      "Size reduction: 0.01 MB -> 0.00 MB (36.2%)\n",
      "\n",
      "Optimizing codec_encoder...\n",
      "Warning: Using ALL optimizations (may include hardware-specific optimizations)\n",
      "Optimized model saved to onnx_models_production/codec_encoder_opt.onnx\n",
      "Size reduction: 72.63 MB -> 72.61 MB (0.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[0;93m2025-06-06 19:55:20.584783 [W:onnxruntime:, inference_session.cc:2212 Initialize] Serializing optimized model with Graph Optimization level greater than ORT_ENABLE_EXTENDED and the NchwcTransformer enabled. The generated model may contain hardware specific optimizations, and should only be used in the same environment the model was optimized in.\u001B[m\n",
      "\u001B[0;93m2025-06-06 19:55:20.640826 [W:onnxruntime:, inference_session.cc:2212 Initialize] Serializing optimized model with Graph Optimization level greater than ORT_ENABLE_EXTENDED and the NchwcTransformer enabled. The generated model may contain hardware specific optimizations, and should only be used in the same environment the model was optimized in.\u001B[m\n",
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n",
      "INFO:root:Quantization parameters for tensor:\"/encoder/encoder.3/Relu_output_0\" not specified\n",
      "\u001B[0;93m2025-06-06 19:55:21.347206 [W:onnxruntime:, inference_session.cc:2212 Initialize] Serializing optimized model with Graph Optimization level greater than ORT_ENABLE_EXTENDED and the NchwcTransformer enabled. The generated model may contain hardware specific optimizations, and should only be used in the same environment the model was optimized in.\u001B[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Quantized: 72.61 MB -> 41.11 MB (43.4%)\n",
      "\n",
      "Optimizing mask_generator...\n",
      "Warning: Using ALL optimizations (may include hardware-specific optimizations)\n",
      "Optimized model saved to onnx_models_production/mask_generator_opt.onnx\n",
      "Size reduction: 0.01 MB -> 0.00 MB (38.2%)\n",
      "\n",
      "Optimizing transformer...\n",
      "Warning: Using ALL optimizations (may include hardware-specific optimizations)\n",
      "Optimized model saved to onnx_models_production/transformer_opt.onnx\n",
      "Size reduction: 38.16 MB -> 38.04 MB (0.3%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[0;93m2025-06-06 19:55:21.448401 [W:onnxruntime:, inference_session.cc:2212 Initialize] Serializing optimized model with Graph Optimization level greater than ORT_ENABLE_EXTENDED and the NchwcTransformer enabled. The generated model may contain hardware specific optimizations, and should only be used in the same environment the model was optimized in.\u001B[m\n",
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n",
      "INFO:root:Quantization parameters for tensor:\"/model/transformer/layers.0/self_attn/Transpose_output_0\" not specified\n",
      "INFO:root:Ignore MatMul due to non constant B: /[/model/transformer/layers.0/self_attn/MatMul_1]\n",
      "INFO:root:Ignore MatMul due to non constant B: /[/model/transformer/layers.0/self_attn/MatMul_2]\n",
      "INFO:root:Quantization parameters for tensor:\"/model/transformer/layers.0/self_attn/Reshape_9_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/model/transformer/layers.0/norm1/Add_1_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/model/transformer/layers.0/Relu_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/model/transformer/layers.1/self_attn/Transpose_output_0\" not specified\n",
      "INFO:root:Ignore MatMul due to non constant B: /[/model/transformer/layers.1/self_attn/MatMul_1]\n",
      "INFO:root:Ignore MatMul due to non constant B: /[/model/transformer/layers.1/self_attn/MatMul_2]\n",
      "INFO:root:Quantization parameters for tensor:\"/model/transformer/layers.1/self_attn/Reshape_9_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/model/transformer/layers.1/norm1/Add_1_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/model/transformer/layers.1/Relu_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/model/transformer/layers.2/self_attn/Transpose_output_0\" not specified\n",
      "INFO:root:Ignore MatMul due to non constant B: /[/model/transformer/layers.2/self_attn/MatMul_1]\n",
      "INFO:root:Ignore MatMul due to non constant B: /[/model/transformer/layers.2/self_attn/MatMul_2]\n",
      "INFO:root:Quantization parameters for tensor:\"/model/transformer/layers.2/self_attn/Reshape_9_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/model/transformer/layers.2/norm1/Add_1_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/model/transformer/layers.2/Relu_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/model/transformer/layers.3/self_attn/Transpose_output_0\" not specified\n",
      "INFO:root:Ignore MatMul due to non constant B: /[/model/transformer/layers.3/self_attn/MatMul_1]\n",
      "INFO:root:Ignore MatMul due to non constant B: /[/model/transformer/layers.3/self_attn/MatMul_2]\n",
      "INFO:root:Quantization parameters for tensor:\"/model/transformer/layers.3/self_attn/Reshape_9_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/model/transformer/layers.3/norm1/Add_1_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/model/transformer/layers.3/Relu_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/model/transformer/layers.4/self_attn/Transpose_output_0\" not specified\n",
      "INFO:root:Ignore MatMul due to non constant B: /[/model/transformer/layers.4/self_attn/MatMul_1]\n",
      "INFO:root:Ignore MatMul due to non constant B: /[/model/transformer/layers.4/self_attn/MatMul_2]\n",
      "INFO:root:Quantization parameters for tensor:\"/model/transformer/layers.4/self_attn/Reshape_9_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/model/transformer/layers.4/norm1/Add_1_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/model/transformer/layers.4/Relu_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/model/transformer/layers.5/self_attn/Transpose_output_0\" not specified\n",
      "INFO:root:Ignore MatMul due to non constant B: /[/model/transformer/layers.5/self_attn/MatMul_1]\n",
      "INFO:root:Ignore MatMul due to non constant B: /[/model/transformer/layers.5/self_attn/MatMul_2]\n",
      "INFO:root:Quantization parameters for tensor:\"/model/transformer/layers.5/self_attn/Reshape_9_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/model/transformer/layers.5/norm1/Add_1_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/model/transformer/layers.5/Relu_output_0\" not specified\n",
      "INFO:root:Quantization parameters for tensor:\"/model/transformer/layers.5/norm2/Add_1_output_0\" not specified\n",
      "\u001B[0;93m2025-06-06 19:55:21.978152 [W:onnxruntime:, inference_session.cc:2212 Initialize] Serializing optimized model with Graph Optimization level greater than ORT_ENABLE_EXTENDED and the NchwcTransformer enabled. The generated model may contain hardware specific optimizations, and should only be used in the same environment the model was optimized in.\u001B[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Quantized: 38.04 MB -> 17.07 MB (55.1%)\n",
      "\n",
      "Optimizing codec_decoder...\n",
      "Warning: Using ALL optimizations (may include hardware-specific optimizations)\n",
      "Optimized model saved to onnx_models_production/codec_decoder_opt.onnx\n",
      "Size reduction: 100.52 MB -> 100.52 MB (0.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Quantized: 100.52 MB -> 79.53 MB (20.9%)\n",
      "\n",
      "Optimized pipeline created in onnx_models_production\n"
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
